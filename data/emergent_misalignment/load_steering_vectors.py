"""
Load steering vectors from all checkpoints of the steering vector models.

This script loads the steering vectors from all available checkpoints
and saves them for analysis.
"""

import json
from pathlib import Path
from typing import Any, Dict

import torch
from dotenv import load_dotenv
from tqdm import tqdm  # type: ignore

from data.emergent_misalignment.dissecting_em_utils import get_all_checkpoint_components
from data.emergent_misalignment.em_utils import get_hf_names

load_dotenv()


def load_all_steering_vectors_from_model(repo_id: str, output_dir: str) -> Dict[str, Any]:
    """
    Load steering vectors from all checkpoints of a steering vector model.

    Args:
        repo_id: HuggingFace repository ID
        output_dir: Directory to save the vectors

    Returns:
        Dictionary containing metadata about the loaded vectors
    """
    print(f"\nLoading steering vectors from {repo_id}...")

    # Create output directory
    model_name = repo_id.split("/")[-1]
    model_output_dir = Path(output_dir) / model_name
    model_output_dir.mkdir(parents=True, exist_ok=True)

    # Use the existing utility to load all checkpoint components
    print("Loading all checkpoint components...")
    try:
        all_components = get_all_checkpoint_components(repo_id, quiet=False)
    except Exception as e:
        print(f"  Error loading checkpoint components: {e}")
        raise

    print(f"Found {len(all_components)} checkpoints")

    if len(all_components) == 0:
        print("  Warning: No checkpoints found")
        return {
            "repo_id": repo_id,
            "model_name": model_name,
            "num_checkpoints": 0,
            "checkpoints": [],
            "vectors": {},
            "error": "No checkpoints found",
        }

    # Extract steering vectors from each checkpoint
    vectors_data = {}
    metadata = {
        "repo_id": repo_id,
        "model_name": model_name,
        "num_checkpoints": len(all_components),
        "checkpoints": sorted(all_components.keys()),
        "vectors": {},
    }

    print("Extracting steering vectors from checkpoints...")
    for checkpoint_name, components in tqdm(all_components.items(), desc=f"Processing {model_name}"):
        try:
            # For steering vector models, the components should have a "steering_layer" key
            # and the B component contains the steering vector
            if "steering_layer" in components.components:
                steering_component = components.components["steering_layer"]
                steering_vector = steering_component.B

                # Check if steering_vector is not None
                if steering_vector is None:
                    print(f"  Warning: Steering vector is None in {checkpoint_name}")
                    continue

                # Save the vector
                checkpoint_num = checkpoint_name.split("_")[-1]
                vector_file = model_output_dir / f"steering_vector_checkpoint_{checkpoint_num}.pt"
                torch.save(steering_vector, vector_file)

                # Store metadata
                vectors_data[checkpoint_name] = {
                    "file_path": str(vector_file),
                    "shape": list(steering_vector.shape),
                    "norm": float(torch.norm(steering_vector).item()),
                    "checkpoint": checkpoint_num,
                }
            else:
                print(f"  Warning: No steering_layer found in {checkpoint_name}")
                continue

        except Exception as e:
            print(f"  Warning: Failed to process checkpoint {checkpoint_name}: {e}")
            continue

    # Save metadata
    metadata["vectors"] = vectors_data
    metadata_file = model_output_dir / "metadata.json"
    with open(metadata_file, "w") as f:
        json.dump(metadata, f, indent=2)

    print(f"Saved {len(vectors_data)} steering vectors to {model_output_dir}")
    return metadata


def main():
    """Load steering vectors from all models."""
    # Get model repositories from centralized configuration
    models = get_hf_names()

    # Output directory
    output_dir = "data/emergent_misalignment/steering_vectors"

    print("Loading steering vectors from steering vector models...")
    print("=" * 60)
    print(f"Found {len(models)} models to process")

    all_metadata = {}

    for model_repo in models:
        try:
            print(f"\nProcessing model: {model_repo}")
            metadata = load_all_steering_vectors_from_model(repo_id=model_repo, output_dir=output_dir)
            all_metadata[model_repo] = metadata
        except Exception as e:
            print(f"Error loading from {model_repo}: {e}")
            continue

    # Save overall metadata
    overall_metadata_file = Path(output_dir) / "overall_metadata.json"
    with open(overall_metadata_file, "w") as f:
        json.dump(all_metadata, f, indent=2)

    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)

    total_models = len(models)
    successful_models = len(all_metadata)
    print(f"Successfully processed: {successful_models}/{total_models} models")

    for model_repo, metadata in all_metadata.items():
        model_name = metadata["model_name"]
        num_checkpoints = metadata["num_checkpoints"]
        num_loaded = len(metadata["vectors"])
        print(f"{model_name}: {num_loaded}/{num_checkpoints} checkpoints loaded")

    print(f"\nAll data saved to: {output_dir}")
    print(f"Overall metadata: {overall_metadata_file}")


if __name__ == "__main__":
    main()
